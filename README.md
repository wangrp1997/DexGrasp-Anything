<br>
<p align="center">
<h1 align="center"><strong> DexGraspAnything:TowardsUniversalRoboticDexterousGrasping
 withPhysicsAwareness
</strong></h1>
  <p align="center">
      <strong><span style="color: red;">CVPR 2025</span></strong>
    <br>
   <a href='https://ymzhong66.github.io' target='_blank'>Yiming Zhong*</a>&emsp;
   <a href='https://github.com/Kenny-K' target='_blank'>Qi Jiang*</a>&emsp;
   <a href='https://sist.shanghaitech.edu.cn/yujingyi/main.htm' target='_blank'>Jingyi Yu</a>&emsp;
   <a href='https://yuexinma.me' target='_blank'>Yuexin Ma</a>&emsp;
    <br>
    ShanghaiTech University    
    <br>
    *Indicates Equal Contribution
    <br>
  </p>
</p>

  

<p align="center">
  <a href="https://dexgraspanything.github.io/"><b>ğŸ“– Project Page</b></a> |
  <a href="https://dexgraspanything.github.io/"><b>ğŸ“„ Paper Link</b></a> |
</p>

</div>

>  We present DexGrasp Anything, consistently surpassing previous dexterous grasping generation methods across five benchmarks. Visualization of our method's results are shown on the left.

<div align="center">
    <img src="image1.png" height=500>
</div>

## ğŸ“£ News
- [2/27/2025] ğŸ‰ğŸ‰ğŸ‰DexGraspAnything has been accepted by CVPR 2025!!!ğŸ‰ğŸ‰ğŸ‰

## ğŸ˜² Results
Please refer to our [homepage](https://dexgraspanything.github.io/) for more thrilling results!


## ğŸ› ï¸ Setup
- Comming Soon...


## ğŸš© Plan
- [ ] Paper Released.
- [ ] Source Code and Pretrained Weights.
- [ ] Dataset.
<!-- --- -->



## ğŸ« License

For academic use, this project is licensed under [the 2-clause BSD License](https://opensource.org/license/bsd-2-clause). 

## ğŸ–Šï¸ Citation
<!-- ```
@article{yu2024seqafford,
        title={SeqAfford: Sequential 3D Affordance Reasoning via Multimodal Large Language Model},
        author={Yu, Chunlin and Wang, Hanqing and Shi, Ye and Luo, Haoyang and Yang, Sibei and Yu, Jingyi and Wang, Jingya},
        journal={arXiv preprint arXiv:2412.01550},
        year={2024}
      }

``` -->

